{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81f116eb-2a93-4c63-bb1c-2f01b66c931e",
   "metadata": {},
   "source": [
    "1. Write a PySpark code to read a CSV file named \"employees.csv\" containing the following columns: \"employee_id\", \"name\", \"age\", \"department\". Display the top 10 records from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4ed8ea-5646-4036-9d16-417b6b332446",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(\"employees.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Display the top 10 records\n",
    "df.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464e8713-bc4b-4bd1-bd8e-a5fe3851b00f",
   "metadata": {},
   "source": [
    "2.  Given a PySpark DataFrame named \"sales_data\" with columns \"product_name\" and \"revenue\", write a code to calculate the total revenue for each product and display the result in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8799a8-d5b5-4c2b-aa73-274817e91db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Assuming you have the \"sales_data\" DataFrame with columns \"product_name\" and \"revenue\"\n",
    "\n",
    "# Calculate total revenue for each product\n",
    "total_revenue_df = sales_data.groupBy(\"product_name\").agg(sum(\"revenue\").alias(\"total_revenue\"))\n",
    "\n",
    "# Display the result in descending order\n",
    "total_revenue_df = total_revenue_df.orderBy(\"total_revenue\", ascending=False)\n",
    "\n",
    "# Show the result\n",
    "total_revenue_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab1d0f7-37ba-4bf3-9674-56e93e10b27f",
   "metadata": {},
   "source": [
    "3. Write a PySpark code to read a JSON file named \"students.json\" containing student records with the following schema: \"name\" (string), \"age\" (integer), \"grade\" (string). Filter the DataFrame to include only students whose age is greater than 18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae56a35-a3ae-46ed-abcb-499b7c53c1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Read the JSON file into a DataFrame\n",
    "df = spark.read.json(\"students.json\")\n",
    "\n",
    "# Filter the DataFrame to include only students whose age is greater than 18\n",
    "filtered_df = df.filter(col(\"age\") > 18)\n",
    "\n",
    "# Show the filtered DataFrame\n",
    "filtered_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1af697-16d6-4ad6-961f-0ba46539e90d",
   "metadata": {},
   "source": [
    "4. Consider a PySpark DataFrame named \"transactions\" with columns \"transaction_id\", \"user_id\", and \"amount\". Write a code to calculate the average transaction amount for each user and display the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0345b66e-bec8-4c0d-91ab-3b4f72b29271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Assuming you have the \"transactions\" DataFrame with columns \"transaction_id\", \"user_id\", and \"amount\"\n",
    "\n",
    "# Calculate the average transaction amount for each user\n",
    "avg_amount_df = transactions.groupBy(\"user_id\").agg(avg(\"amount\").alias(\"avg_amount\"))\n",
    "\n",
    "# Display the result\n",
    "avg_amount_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5b4e31-35e4-47e2-9060-5c7df778f229",
   "metadata": {},
   "source": [
    "5. Given a PySpark DataFrame named \"logs\" with columns \"timestamp\" (timestamp) and \"event\" (string), write a code to count the number of events that occurred in each hour and display the result sorted by the hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df9c949-d8ef-4d9c-a5cb-4b73c6af2646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import hour\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Assuming you have the \"logs\" DataFrame with columns \"timestamp\" (timestamp) and \"event\" (string)\n",
    "\n",
    "# Extract the hour from the timestamp column\n",
    "logs_with_hour = logs.withColumn(\"hour\", hour(\"timestamp\"))\n",
    "\n",
    "# Count the number of events in each hour\n",
    "event_count_df = logs_with_hour.groupBy(\"hour\").count().orderBy(\"hour\")\n",
    "\n",
    "# Display the result\n",
    "event_count_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2cf2ad-232b-4b59-9ddb-2d511391dfba",
   "metadata": {},
   "source": [
    "6.  Retrieve all the customers from the \"Customers\" table whose age is greater than 25 and have made at least one purchase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a13918-c912-4243-9e63-3a8eb48265ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Assuming you have a table named \"Customers\" in your database\n",
    "\n",
    "# Execute a SQL query to retrieve the desired customers\n",
    "query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM Customers\n",
    "    WHERE age > 25\n",
    "    AND customer_id IN (\n",
    "        SELECT customer_id\n",
    "        FROM Purchases\n",
    "    )\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and retrieve the result as a DataFrame\n",
    "result_df = spark.sql(query)\n",
    "\n",
    "# Display the result\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6792f05d-1d70-4aa0-a6ef-b23f8e06274d",
   "metadata": {},
   "source": [
    "7. Find the total number of orders placed by each customer and display the results in descending order of the number of orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e580724-82b5-42ea-ae06-e9020809db1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Assuming you have a table named \"Orders\" with columns \"customer_id\" and \"order_id\"\n",
    "\n",
    "# Execute a SQL query to find the total number of orders placed by each customer\n",
    "query = \"\"\"\n",
    "    SELECT customer_id, COUNT(order_id) as order_count\n",
    "    FROM Orders\n",
    "    GROUP BY customer_id\n",
    "    ORDER BY order_count DESC\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and retrieve the result as a DataFrame\n",
    "result_df = spark.sql(query)\n",
    "\n",
    "# Display the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3580b92-df6d-4d78-a4f5-83e29ff1dcf3",
   "metadata": {},
   "source": [
    "8. Retrieve the names of all products that are currently out of stock from the \"Products\" table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa28515e-0201-4436-884d-1d2aa16bdb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Assuming you have a table named \"Products\" with columns \"product_id\" and \"product_name\"\n",
    "\n",
    "# Execute a SQL query to retrieve the names of out-of-stock products\n",
    "query = \"\"\"\n",
    "    SELECT product_name\n",
    "    FROM Products\n",
    "    WHERE stock_quantity = 0\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and retrieve the result as a DataFrame\n",
    "result_df = spark.sql(query)\n",
    "\n",
    "# Display the result\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fe8da4-41b5-468d-b338-c0e5bab579e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
